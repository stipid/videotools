{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stipid/videotools/blob/main/whisper_timestamped_for_driver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q0jEeQOvZmK"
      },
      "source": [
        "*italicized text*# WhisperTimestamped for uploading files\n",
        "\n",
        "Upload your local files to the Colab Files from the left sidebar.\n",
        "\n",
        "从左侧将视频音频文件上传到Colab，然后运行即可下载生成好的字幕文件。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9Pgww7RuFUW",
        "outputId": "0cdd252b-dc50-4115-83a9-81dfbabbe898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 16 23:34:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# @title *通用参数/Required settings:**\n",
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# @markdown #### **Initial prompt**\n",
        "# @markdown Prompts can be very helpful for correcting specific words or acronyms that the model often misrecognizes in the audio.\n",
        "prompt = \"ChatGPT, LLM, OpenAI, DevDay, DALL-E.\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### **Directory Path**\n",
        "# @markdown where your audio-video files are located from Coloab\n",
        "# directory_path = \"/content\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown #### Model\n",
        "model_size = \"large-v3\"  # @param [\"base\", \"base.en\", \"small\", \"small.en\",\"medium\", \"medium.en\", \"large-v1\",\"large-v2\",\"large-v3\"]\n",
        "\n",
        "# @markdown #### Language\n",
        "language = \"en\" # @param [\"auto\", \"en\", \"zh\", \"de\", \"es\", \"ru\", \"ko\", \"fr\", \"ja\", \"pt\", \"tr\", \"pl\", \"ca\", \"nl\", \"ar\", \"sv\", \"it\", \"id\", \"hi\", \"fi\", \"vi\", \"he\", \"uk\", \"el\", \"ms\", \"cs\", \"ro\", \"da\", \"hu\", \"ta\", \"no\", \"th\", \"ur\", \"hr\", \"bg\", \"lt\", \"la\", \"mi\", \"ml\", \"cy\", \"sk\", \"te\", \"fa\", \"lv\", \"bn\", \"sr\", \"az\", \"sl\", \"kn\", \"et\", \"mk\", \"br\", \"eu\", \"is\", \"hy\", \"ne\", \"mn\", \"bs\", \"kk\", \"sq\", \"sw\", \"gl\", \"mr\", \"pa\", \"si\", \"km\", \"sn\", \"yo\", \"so\", \"af\", \"oc\", \"ka\", \"be\", \"tg\", \"sd\", \"gu\", \"am\", \"yi\", \"lo\", \"uz\", \"fo\", \"ht\", \"ps\", \"tk\", \"nn\", \"mt\", \"sa\", \"lb\", \"my\", \"bo\", \"tl\", \"mg\", \"as\", \"tt\", \"haw\", \"ln\", \"ha\", \"ba\", \"jw\", \"su\"]\n",
        "\n",
        "# @markdown #### Filename Type\n",
        "# @markdown Use YouTube title as file name by default\n",
        "filename_type = \"title\"  # @param [\"title\", \"id\"]\n",
        "\n",
        "# @markdown #### Assign speaker labels\n",
        "# @markdown Recognize speakers\n",
        "assign_speaker_lable = False # @param {type:\"boolean\"}\n",
        "\n",
        "# @markdown #### Align whisper output\n",
        "align_whisper_output = False # @param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80yOVjb-vAsD"
      },
      "source": [
        "# Install Whisper Timestamped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcc35ui3ux8l",
        "outputId": "dd82e3bb-2229-4f7b-dc70-69c8cd81997b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/linto-ai/whisper-timestamped\n",
            "  Cloning https://github.com/linto-ai/whisper-timestamped to /tmp/pip-req-build-xezs7zkw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/linto-ai/whisper-timestamped /tmp/pip-req-build-xezs7zkw\n",
            "  Resolved https://github.com/linto-ai/whisper-timestamped to commit e495276f69f5ee31caa1209e7a05dae4442be5e4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from whisper-timestamped==1.15.6) (3.0.11)\n",
            "Collecting dtw-python (from whisper-timestamped==1.15.6)\n",
            "  Downloading dtw_python-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai-whisper (from whisper-timestamped==1.15.6)\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from dtw-python->whisper-timestamped==1.15.6) (1.13.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from dtw-python->whisper-timestamped==1.15.6) (1.26.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper-timestamped==1.15.6) (0.60.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper-timestamped==1.15.6) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper-timestamped==1.15.6) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper->whisper-timestamped==1.15.6) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper->whisper-timestamped==1.15.6)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper->whisper-timestamped==1.15.6)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper->whisper-timestamped==1.15.6) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper->whisper-timestamped==1.15.6) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper->whisper-timestamped==1.15.6) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper->whisper-timestamped==1.15.6) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper-timestamped==1.15.6) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper-timestamped==1.15.6) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper-timestamped==1.15.6) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper-timestamped==1.15.6) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper->whisper-timestamped==1.15.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper->whisper-timestamped==1.15.6) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper-timestamped==1.15.6) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper-timestamped==1.15.6) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper-timestamped==1.15.6) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper->whisper-timestamped==1.15.6) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper->whisper-timestamped==1.15.6) (3.0.2)\n",
            "Downloading dtw_python-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.7/764.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: whisper-timestamped, openai-whisper\n",
            "  Building wheel for whisper-timestamped (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for whisper-timestamped: filename=whisper_timestamped-1.15.6-py3-none-any.whl size=53657 sha256=399d7cab4f51d1913c786976933c80d0cfc146cd5f4cf9351757b6ff90d52a53\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4vx3n3wo/wheels/9d/fb/03/ae0b1c8b71edda71e33da77333c78117d879361bdd35de48e4\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=fd37c32fea91dfff8d2a48c629217ae4ce782fc9612c378a7fefe0fcd874f46b\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built whisper-timestamped openai-whisper\n",
            "Installing collected packages: triton, tiktoken, dtw-python, openai-whisper, whisper-timestamped\n",
            "Successfully installed dtw-python-1.5.3 openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0 whisper-timestamped-1.15.6\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# add 20241029 refer https://github.com/ufal/whisper_streaming/issues/129\n",
        "# ! pip install openai-whisper==20231117\n",
        "\n",
        "! pip install git+https://github.com/linto-ai/whisper-timestamped\n",
        "! pip install matplotlib\n",
        "\n",
        "# ! pip install git+https://github.com/openai/whisper.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DJXxWPzU7c6",
        "outputId": "96740b01-8695-46cb-8e8d-2145d051215e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "# Please BE VERY CAREFUL, this will link your entire drive.\n",
        "# So don't edit code, except the one that says 'Customize the following options',\n",
        "# or you might mess up your files.\n",
        "# IF YOU DO NO WANT TO LINK DRIVE, please see below for an alternative!\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxEqkpyuQL5_"
      },
      "source": [
        "# Get Subtitle Files using WhisperX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWk9Y3Uxv9qu",
        "outputId": "db1380c8-cde3-4c26-d611-f4635da3a1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiaryoaWimpyKid06.mp3\n",
            "whisper_timestamped \"/gdrive/MyDrive/media_proc/speedreg/in/DiaryoaWimpyKid06.mp3\" --model large-v3 --language en --output_dir /gdrive/MyDrive/media_proc/speedreg/out/DiaryoaWimpyKid06.mp3 --initial_prompt \"ChatGPT, LLM, OpenAI, DevDay, DALL-E.\"   --output_format \"srt,json,vtt\"\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "100% 19696/19696 [01:14<00:00, 265.71frames/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "# from google.colab import files\n",
        "\n",
        "in_path = '/gdrive/MyDrive/media_proc/speedreg/in/'\n",
        "out_path = '/gdrive/MyDrive/media_proc/speedreg/out/'\n",
        "\n",
        "directory_path = in_path\n",
        "\n",
        "# supported extensions\n",
        "supported_extensions = ['.mp4', '.wav', '.mp3']\n",
        "\n",
        "language_param = \"\"\n",
        "if language != \"auto\":\n",
        "    language_param = f\"--language {language}\"\n",
        "\n",
        "diarize_param = \"\"\n",
        "if assign_speaker_lable:\n",
        "    diarize_param = \"--diarize --hf_token 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\"\n",
        "\n",
        "align_whisper_param = \"\"\n",
        "# if align_whisper_output:\n",
        "#     align_whisper_param = \"--align_model WAV2VEC2_ASR_LARGE_LV60K_960H\"\n",
        "\n",
        "prompt_param = \"\"\n",
        "if prompt != \"\":\n",
        "    prompt_param = f'--initial_prompt \"{prompt}\"'\n",
        "\n",
        "import os, shutil\n",
        "def calculate_relative_path(directory_path, filename):\n",
        "    # 获取指定目录的绝对路径\n",
        "    abs_directory_path = os.path.abspath(directory_path)\n",
        "\n",
        "    # 获取给定文件名的绝对路径\n",
        "    abs_file_path = os.path.abspath(filename)\n",
        "\n",
        "    # 确保给定的文件名在指定目录下的子目录树中\n",
        "    if not abs_file_path.startswith(abs_directory_path):\n",
        "        raise ValueError(\"The given filename is not in the specified directory tree.\")\n",
        "\n",
        "    # 计算相对路径\n",
        "    relative_path = os.path.relpath(abs_file_path, abs_directory_path)\n",
        "    return relative_path\n",
        "\n",
        "def make_out_relative_path(out_path, filename):\n",
        "    # 计算出filename的路径在directory_path下的子目录树\n",
        "    relative_path = calculate_relative_path(in_path, filename)\n",
        "    print(relative_path)\n",
        "    existing_directory = os.path.join(out_path, relative_path)\n",
        "    if os.path.exists(existing_directory):\n",
        "        shutil.rmtree(existing_directory)\n",
        "\n",
        "    # 在另一个目录下创建子目录\n",
        "    new_directory = os.path.join(out_path, relative_path)\n",
        "    os.makedirs(new_directory, exist_ok=True)\n",
        "    return new_directory\n",
        "\n",
        "def process_file(filename):\n",
        "\n",
        "    new_directory = make_out_relative_path(out_path, filename)\n",
        "    # run = f'whisper_timestamped \"/content/APO2992689654.mp3\" --max_line_count 1 --max_line_width 100 --model medium.en --diarize --hf_token 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 --output_dir . --align_model WAV2VEC2_ASR_LARGE_LV60K_960H'\n",
        "    output_format = '--output_format \"srt,json,vtt\"'\n",
        "    run = f'whisper_timestamped \"{filename}\" --model {model_size} {language_param} --output_dir {new_directory} {prompt_param} {diarize_param} {align_whisper_param} {output_format}'\n",
        "    print(run)\n",
        "    !{run}\n",
        "\n",
        "    # print(\"Start to download subtitle files\")\n",
        "    # # start to download file\n",
        "    # base_filename = os.path.splitext(filename)[0]\n",
        "    # srt_filename = base_filename + '.srt'\n",
        "    # json_filename = base_filename + '.words.json'\n",
        "\n",
        "    # files.download(srt_filename)\n",
        "    # files.download(json_filename)\n",
        "\n",
        "def process():\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for file in files:\n",
        "            if file.endswith(tuple(supported_extensions)):\n",
        "                file_path = os.path.join(root, file)\n",
        "                process_file(file_path)\n",
        "\n",
        "process()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}